---
title: "MACHINE LEARNING PROJECT"
author: "VINCENT JUNIOR"
date: "8/1/2020"
output: html_document
---

***
# Part 1: GOAL OF THE PROJECT

## BACKGROUND
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now
possible to collect a large amount of data about personal activity
relatively inexpensively. These type of devices are part of the
quantified self movement â€“ a group of enthusiasts who take measurements
about themselves regularly to improve their health, to find patterns in
their behavior, or because they are tech geeks. One thing that people
regularly do is quantify how much of a particular activity they do, but
they rarely quantify how well they do it. In this project, the goal will
be to use data from accelerometers on the belt, forearm, arm, and dumbell
of 6 participants. They were asked to perform barbell lifts correctly and
incorrectly in 5 different ways. More information is available from the
website [here](http://groupware.les.inf.puc-rio.br/har)

## GOAL

Creating a report describing
* how the model is built
* how you cross validation was used
* what I think the expected out of sample error is
* why I made the choices I did

***
# PART 2: GETTING AND CLEANING OF DATA

## DOWNLOADING DATASETS

```{r, cache=TRUE, message=FALSE}

trainingName <- 'training.csv'; testingName <- 'testing.csv'
trainingUrl<- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
testingUrl <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'

download.file(trainingUrl, trainingName)
download.file(testingUrl,testingName)
downloaddate <-date()
```

## LOADING AND CLEANING THE DATA

The data has some missing values. We are going to eliminate them.

```{r, cache=TRUE}
training <- read.csv("training.csv", header = TRUE, sep = ",", na.strings=c("NA","","#DIV/0!"))
training <-training[,colSums(is.na(training)) == 0]

testing <- read.csv("testing.csv", header = TRUE, sep = ",", na.strings=c("NA", "", "#DIV/0!"))
testing <-testing[,colSums(is.na(testing)) == 0]

any(is.na(training)); any(is.na(testing))
```

All missing values are removed. 

```{r}
names(training); names(testing)
```

We are going to remove the first five valuables since they identity valuables. 

```{r}
training <- training[, -c(1:7)]; testing <- testing[, -c(1:7)]

dim(testing); dim(training)
```

We have 53 valuable for this analysis. 


## PARTITIONING THE TRAINING DATASET

```{r}
library(caret)
intrain <- createDataPartition(training$classe, p=.7, list = FALSE)
train <-training[intrain,]
test <- training[-intrain, ]

```


***
# PART 3: PREDICTION MODEL

In this part, i will fit models using different methods then pick the oe with the highest accuracy. 

## PREDICTION USING DECISION TREE

I will fit a prediction model using decision tree and check its accuracy. 

```{r, cache=TRUE, message=FALSE}
set.seed(1234)

library(rpart)

##model fitting
modtree <- rpart(classe ~ ., data = train, method="class")

##Accuracy check
predtree <- predict(modtree, newdata = test, type="class")
confusionMatrix(predtree, test$classe)$overall["Accuracy"] 

```

The Accuracy for this model is about 74%. 

## PREDICTIONS USING RANDOM FOREST.

I will fit a prediction model using random forest and check its accuracy.  


```{r, cache=TRUE}
set.seed(5678)
library(caret)

##Model fitting
   modrf <- train(classe ~ ., data = train,method = "rf", metric = "Accuracy", preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 3))

##Accuracy check
predrf <- predict(modrf, newdata=test)
confusionMatrix(predrf, test$classe)$overall["Accuracy"]

```

The accuracy for random forest is almost 100% so we pick it as the best algorithm for prediction as compared to prediction with trees. 


```{r, cache=TRUE}
confusionMatrix(predrf, test$classe)
```

***

#PART 3: APPLICATION OF THE BEST MODEL TO TESTING DATASET.

The best model, that is the one with the highest accuracy is applied on the testing dataset, that is the dataset for the quiz. This simply verifies how best the model is. 

```{r}
predict(modrf, newdata=testing)
```

The prediction is 100% correct. 
